{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion: Topic Modeling\n",
    "\n",
    "## Group Names and Roles\n",
    "\n",
    "- Chris\n",
    "- Zach\n",
    "\n",
    "## Intro\n",
    "\n",
    "In this Discussion activity, we'll continue with with topic modeling. Recall that topic modeling can often be used to infer themes (or \"topics\") from sets of text data. Today, we will work through an example in which we download some data, prepare it appropriately, and deploy a topic model to obtain insights about the general themes present in the data. \n",
    "\n",
    "Our data set for this activity consists of the texts of a number of Associated Press articles. It was originally collected by David Blei. I retrieved this data set [here](https://github.com/tdhopper/topic-modeling-datasets/tree/master/data/lda-c/blei-ap). \n",
    "\n",
    "Run the following code chunk to create a large string `s` containing the entire data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "def retrieve_text(url):\n",
    "    \"\"\"\n",
    "    Retrieve text from the specified url and return \n",
    "    it as a string\n",
    "    \"\"\"\n",
    "    \n",
    "    # grab the data and parse it\n",
    "    filedata = urllib.request.urlopen(url) \n",
    "    data = filedata.read()\n",
    "    \n",
    "    return(data.decode())\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/PhilChodrow/PIC16A/master/datasets/blei-ap.txt'\n",
    "s = retrieve_text(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Inspect `s`. Don't print out the entire string; just take a look at the first 5,000 characters or so. Write a function which, when `s` is provided as input, will return a list of document texts. It should exclude the excess tags and other metadata. Call this function to create a new list variable called `texts`, where each element of `texts` is the complete text of one news story. Filter your list to include only texts with at least 100 characters. The resulting list should have length 2226. \n",
    "\n",
    "\n",
    "- **Hint**: while one can try to do this with regular expressions, I found it easier to keep things low tech. Is there a relevant character that you can `str.split()` on? Can you remove the entries that you don't want using a conditional list comprehension? \n",
    "\n",
    "Comments and docstrings are not necessary for this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your function and call it on s\n",
    "#s.split(\"<DOC>\\n<DOCNO> AP881218-0003 </DOCNO>\\n<TEXT>\\n\")\n",
    "#s[0:5000]\n",
    "def function(s):\n",
    "    txt = s.split(\"\\n\")\n",
    "    filt = [x for x in txt if len(x) >= 100]\n",
    "    return filt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2226"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check len(texts): should be 2226\n",
    "len(function(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Create a `pandas` data frame called `df` with a single column called `text`, whose rows are the entries of `texts`. This data frame should have 2226 rows. Show your data frame to check that it looks ok. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 16-year-old student at a private Baptist sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Bechtel Group Inc. offered in 1985 to sel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A gunman took a 74-year-old woman hostage aft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today is Saturday, Oct. 29, the 303rd day of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cupid has a new message for lovers this Valen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0   A 16-year-old student at a private Baptist sc...\n",
       "1   The Bechtel Group Inc. offered in 1985 to sel...\n",
       "2   A gunman took a 74-year-old woman hostage aft...\n",
       "3   Today is Saturday, Oct. 29, the 303rd day of ...\n",
       "4   Cupid has a new message for lovers this Valen..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "texts = function(s)\n",
    "df = pd.DataFrame(texts,columns=[\"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "Create the term-document matrix. The group's **Reviewer** might want to check the [lecture notes](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/NLP/NLP_2.ipynb) on topic modeling for some code to do this. Add the term-document matrix to `df`. Make sure that the columns are labeled with the relevant word. \n",
    "\n",
    "I found that, for the purposes of the later parts of this exercise, the following arguments to `CountVectorizer` worked well: \n",
    "\n",
    "> `max_df = 100, min_df=0.01, stop_words='english'`\n",
    "\n",
    "However, please feel free to experiment. \n",
    "\n",
    "Call the new data frame with counts `big_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the term document matrix and big_df here\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(max_df=.5, min_df = 0, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D\n",
    "\n",
    "Create an input matrix `X` which is identical to `big_df` but drops the `text` column. Then, create a Nonnegative Matrix Factorization (NMF) model and fit it to `X`. Start with 10 components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train your model here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E\n",
    "\n",
    "The following code (from lecture) will extract the top words within each topic. Run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def top_words(X, model, component, num_words):\n",
    "    \"\"\"\n",
    "    Extract the top words from the specified component \n",
    "    for a topic model trained on data. \n",
    "    X: a term-document matrix, assumed to be a pd.DataFrame\n",
    "    model: a sklearn model with a components_ attribute, e.g. NMF\n",
    "    component: the desired component, specified as an integer. \n",
    "        Must be less than than the total number of components in model\n",
    "    num_words: the number of words to return.\n",
    "    \"\"\"\n",
    "    orders = np.argsort(model.components_, axis = 1)\n",
    "    important_words = np.array(X.columns)[orders]\n",
    "    return important_words[component][-num_words:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this code to investigate the topics constructed by the model. Can you interpret any of them? I was able to find U.S. political party conventions; fluctuations in the price of oil; U.S. / Soviet tensions (some of these articles are quite old); and international finance news, among other things. Show the top words for a few different topics and write in a brief discussion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
